# -*- coding: utf-8 -*-
"""Ham1000-class-weighting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uGUz8s8Z40YTbTyaX11TGbY9_LgxUExI

# Setup
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten, GlobalAveragePooling2D
from tensorflow.keras import regularizers
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import DenseNet201, EfficientNetB3, ResNet50
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, roc_auc_score
import gc
from tensorflow.keras import backend as K
import itertools
from pathlib import Path

from google.colab import drive
drive.mount('/content/drive')

tf.random.set_seed(42)
np.random.seed(42)


USE_AUGMENTATION_AND_OVERSAMPLING = False
print(f"Using augmentation and oversampling: {USE_AUGMENTATION_AND_OVERSAMPLING}")

"""# Preparing data"""

data_path = '/content/dataset'
if not os.path.exists(data_path):
    os.makedirs(data_path)

import zipfile
zip_ref = zipfile.ZipFile("/content/drive/MyDrive/archive.zip", 'r')
zip_ref.extractall(data_path)
zip_ref.close()

csv_path = os.path.join(data_path, 'HAM10000_metadata.csv')
images_part1 = os.path.join(data_path, 'HAM10000_images_part_1')
images_part2 = os.path.join(data_path, 'HAM10000_images_part_2')

df_data = pd.read_csv(csv_path).set_index('image_id')

lesion_type_dict = {
    'nv': 'Melanocytic nevi',
    'mel': 'Melanoma',
    'bkl': 'Benign keratosis',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}

df_data.dx = df_data.dx.astype('category', copy=True)
df_data['label_numeric'] = df_data.dx.cat.codes
df_data['label'] = df_data.dx
df_data['lesion_type'] = df_data.dx.map(lesion_type_dict)

imageid_path_dict = {}

for image_folder in [images_part1, images_part2]:
    if os.path.exists(image_folder):
        for image_file in os.listdir(image_folder):
            if image_file.endswith('.jpg'):
                image_id = os.path.splitext(image_file)[0]
                imageid_path_dict[image_id] = os.path.join(image_folder, image_file)

df_data['path'] = df_data.index.map(lambda x: imageid_path_dict.get(x, None))

df_data = df_data.dropna(subset=['path'])

print(f"Total images loaded: {len(df_data)}")
print("\nDiagnosis distribution:")
dx_counts = df_data['dx'].value_counts()
for dx, count in dx_counts.items():
    print(f"{lesion_type_dict[dx]} ({dx}): {count} images")

"""#  Data Visualization and Split"""

def plot_data_distribution(df, title='Class Distribution'):
    plt.figure(figsize=(12, 6))
    counts = df['dx'].value_counts().sort_index()

    labels = [f"{lesion_type_dict[dx]} ({dx}): {count}" for dx, count in counts.items()]

    ax = sns.barplot(x=counts.index, y=counts.values)

    plt.title(title, fontsize=16)
    plt.xlabel('Diagnosis', fontsize=14)
    plt.ylabel('Count', fontsize=14)
    plt.xticks(range(len(counts)), labels, rotation=45, ha='right')

    for i, count in enumerate(counts.values):
        ax.text(i, count + 20, str(count), ha='center', fontsize=11)

    plt.tight_layout()
    plt.show()

plot_data_distribution(df_data, 'HAM10000 Dataset Class Distribution')

def check_duplicates(df, col='lesion_id'):
    unique_list = df[col].unique().tolist()
    num_duplicates = len(df) - len(unique_list)
    return num_duplicates

num_duplicates = check_duplicates(df_data)
print(f'Duplicate lesion_ids: {num_duplicates} out of {len(df_data)}')

df_train, df_test = train_test_split(df_data, test_size=0.1, stratify=df_data['label_numeric'], random_state=42)

df_test = df_test.reset_index()
df_train = df_train.reset_index()

df_test = df_test.drop_duplicates(subset='lesion_id', keep="first")

test_lesion_ids = set(df_test['lesion_id'].unique())
df_train = df_train[~df_train['lesion_id'].isin(test_lesion_ids)]

fig, axes = plt.subplots(1, 2, figsize=(16, 6))

for i, (df, title) in enumerate([(df_train, 'Training Set'), (df_test, 'Test Set')]):
    counts = df['dx'].value_counts().sort_index()
    ax = axes[i]
    sns.barplot(x=counts.index, y=counts.values, ax=ax)
    ax.set_title(title)
    ax.set_xlabel('Diagnosis')
    ax.set_ylabel('Count')
    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')

    for j, count in enumerate(counts.values):
        ax.text(j, count + 5, str(count), ha='center')

plt.tight_layout()
plt.show()

print(f"Training set: {len(df_train)} images")
print(f"Test set: {len(df_test)} images")

"""# Data Augmentation and Balance Function"""

def calculate_class_weights(y_train):
    from sklearn.utils.class_weight import compute_class_weight
    import numpy as np

    classes = np.unique(y_train)
    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)

    return dict(zip(classes, class_weights))

def balance_data(train_data, oversample=True):
    if not oversample:
        print("Skipping oversampling as USE_AUGMENTATION_AND_OVERSAMPLING=False")
        return train_data

    balanced_train_data = []
    class_counts = train_data['label_numeric'].value_counts()

    nv_indices = train_data[train_data['dx'] == 'nv'].index
    nv_label = list(train_data[train_data['dx'] == 'nv']['label_numeric'].unique())[0]

    for class_label, count in class_counts.items():
        class_data = train_data[train_data['label_numeric'] == class_label]
        class_name = list(train_data[train_data['label_numeric'] == class_label]['dx'].unique())[0]

        if class_label == nv_label:
            balanced_train_data.append(class_data)
            print(f"Class '{class_name}' (original): {len(class_data)} samples")
        else:
            if count < 1500:
                n_samples_needed = 1500
                n_oversample = n_samples_needed - count

                full_copies = n_oversample // count
                remaining = n_oversample % count

                oversampled = pd.concat([class_data] * full_copies) if full_copies > 0 else pd.DataFrame()

                if remaining > 0:
                    additional = class_data.sample(n=remaining, replace=False, random_state=42)
                    oversampled = pd.concat([oversampled, additional])

                final_class_data = pd.concat([class_data, oversampled])
                balanced_train_data.append(final_class_data)
                print(f"Class '{class_name}' (oversampled): {len(final_class_data)} samples (original: {count})")
            else:
                balanced_train_data.append(class_data.sample(n=1500, random_state=42))
                print(f"Class '{class_name}' (downsampled): 1500 samples (original: {count})")

    balanced_data = pd.concat(balanced_train_data)
    return balanced_data.sample(frac=1, random_state=42).reset_index(drop=True)

common_aug_args = {
    "featurewise_center": False,
    "samplewise_center": False,
    "featurewise_std_normalization": False,
    "samplewise_std_normalization": False,
    "zca_whitening": False,
    "rotation_range": 10,
    "zoom_range": 0.1,
    "width_shift_range": 0.1,
    "height_shift_range": 0.1,
    "horizontal_flip": False,
    "vertical_flip": False,
    "fill_mode": 'nearest'
}

if USE_AUGMENTATION_AND_OVERSAMPLING:
    tr_gen_rescale = ImageDataGenerator(rescale=1.0/255.0, **common_aug_args)
    tr_gen_no_rescale = ImageDataGenerator(**common_aug_args)
else:
    tr_gen_rescale = ImageDataGenerator(rescale=1.0/255.0)
    tr_gen_no_rescale = ImageDataGenerator()

ts_gen_rescale = ImageDataGenerator(rescale=1.0/255.0)
ts_gen_no_rescale = ImageDataGenerator()

"""# Models"""

def create_densenet_model(input_shape=(224, 224, 3), num_classes=7):
    base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=input_shape)
    base_model.trainable = False

    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dropout(0.5),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.6),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.7),
        Dense(num_classes, activation='softmax')
    ])

    return model

def create_efficientnet_model(input_shape=(300, 300, 3), num_classes=7):
    base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=input_shape)
    base_model.trainable = False

    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        Dropout(0.5),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.6),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.7),
        Dense(num_classes, activation='softmax')
    ])

    return model

def create_resnet50_model(input_shape=(112, 150, 3), num_classes=7):
    base_model = ResNet50(
        include_top=False,
        input_shape=input_shape,
        pooling='avg',
        weights='imagenet'
    )

    model = Sequential([
        base_model,
        Dropout(0.5),
        Dense(128, activation="relu",kernel_regularizer=regularizers.l2(0.02)),
        Dropout(0.5),
        Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l2(0.02))
    ])

    return model

def compile_model(model):
    opt = Adam(learning_rate=0.001)
    model.compile(
        optimizer=opt,
        loss='categorical_crossentropy',
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )
    return model

learning_rate_reduction = ReduceLROnPlateau(
    monitor='val_accuracy',
    patience=3,
    verbose=1,
    factor=0.5,
    min_lr=0.00001
)

model_configs = {
    "ResNet50": {"img_size": (112, 150), "use_rescale": True, "model_func": create_resnet50_model},
    "DenseNet201": {"img_size": (224, 224), "use_rescale": True, "model_func": create_densenet_model},
    "EfficientNetB3": {"img_size": (300, 300), "use_rescale": False, "model_func": create_efficientnet_model}
}

"""# Functions for Visualization"""

def plot_roc_curves(fpr, tpr, roc_auc, class_names, title='ROC Curves'):
    plt.figure(figsize=(12, 10))

    plt.plot(fpr["micro"], tpr["micro"],
             label=f'micro-average ROC (AUC = {roc_auc["micro"]:.2f})',
             color='deeppink', linestyle=':', linewidth=4)

    plt.plot(fpr["macro"], tpr["macro"],
             label=f'macro-average ROC (AUC = {roc_auc["macro"]:.2f})',
             color='navy', linestyle=':', linewidth=4)

    colors = itertools.cycle(['aqua', 'darkorange', 'cornflowerblue', 'green', 'red', 'purple', 'brown'])
    for i, color in zip(range(len(class_names)), colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2,
                 label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')

    plt.plot([0, 1], [0, 1], 'k--', lw=2)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.grid(True)
    plt.show()

def plot_training_history(history, model_name):
    plt.figure(figsize=(18, 5))

    plt.subplot(1, 3, 1)
    plt.plot(history.history['loss'], label='Training')
    if 'val_loss' in history.history:
        plt.plot(history.history['val_loss'], label='Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'{model_name} - Loss')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 3, 2)
    plt.plot(history.history['accuracy'], label='Training')
    if 'val_accuracy' in history.history:
        plt.plot(history.history['val_accuracy'], label='Validation')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(f'{model_name} - Accuracy')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 3, 3)
    plt.plot(history.history['auc'], label='Training')
    if 'val_auc' in history.history:
        plt.plot(history.history['val_auc'], label='Validation')
    plt.xlabel('Epochs')
    plt.ylabel('AUC')
    plt.title(f'{model_name} - AUC')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

"""# K-Fold Cross-Validation Training"""

num_folds = 5
skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)
num_epochs = 24
batch_size = 32

for model_name, config in model_configs.items():
    print("\n================================================")
    print(f"Starting {num_folds}-Fold CV for model: {model_name}")
    print("================================================")

    img_size = config["img_size"]
    use_rescale = config["use_rescale"]
    model_func = config["model_func"]

    fold_no = 1
    acc_per_fold = []
    loss_per_fold = []
    auc_per_fold = []
    val_loss_histories = []
    val_acc_histories = []
    val_auc_histories = []
    train_loss_histories = []
    train_acc_histories = []
    train_auc_histories = []

    roc_curves = {'fold': [], 'fpr': [], 'tpr': [], 'roc_auc': []}

    for train_index, val_index in skf.split(df_train, df_train['label_numeric']):
        print(f"\n--- Fold {fold_no} ---")
        train_data = df_train.iloc[train_index]
        val_data = df_train.iloc[val_index]

        print("\nBalancing training data...")
        balanced_train_data = balance_data(train_data, oversample=USE_AUGMENTATION_AND_OVERSAMPLING)
        print(f"Final balanced training set size: {len(balanced_train_data)} samples")

        class_weights = calculate_class_weights(balanced_train_data['label_numeric'])
        print("Class weights:", class_weights)

        if use_rescale:
            train_gen_fold = tr_gen_rescale.flow_from_dataframe(
                balanced_train_data,
                x_col='path',
                y_col='label',
                target_size=img_size,
                class_mode='categorical',
                color_mode='rgb',
                shuffle=True,
                batch_size=batch_size
            )
            valid_gen_fold = ts_gen_rescale.flow_from_dataframe(
                val_data,
                x_col='path',
                y_col='label',
                target_size=img_size,
                class_mode='categorical',
                color_mode='rgb',
                shuffle=False,
                batch_size=batch_size
            )
        else:
            train_gen_fold = tr_gen_no_rescale.flow_from_dataframe(
                balanced_train_data,
                x_col='path',
                y_col='label',
                target_size=img_size,
                class_mode='categorical',
                color_mode='rgb',
                shuffle=True,
                batch_size=batch_size
            )
            valid_gen_fold = ts_gen_no_rescale.flow_from_dataframe(
                val_data,
                x_col='path',
                y_col='label',
                target_size=img_size,
                class_mode='categorical',
                color_mode='rgb',
                shuffle=False,
                batch_size=batch_size
            )

        class_indices = train_gen_fold.class_indices
        class_names = list(class_indices.keys())

        model = model_func(input_shape=(img_size[0], img_size[1], 3), num_classes=len(class_indices))
        model = compile_model(model)

        if fold_no == 1:
            model.summary()

        history = model.fit(
            train_gen_fold,
            epochs=num_epochs,
            validation_data=valid_gen_fold,
            callbacks=[learning_rate_reduction],
            class_weight=class_weights,
            verbose=1
        )

        train_loss_histories.append(history.history['loss'])
        train_acc_histories.append(history.history['accuracy'])
        train_auc_histories.append(history.history['auc'])
        val_loss_histories.append(history.history['val_loss'])
        val_acc_histories.append(history.history['val_accuracy'])
        val_auc_histories.append(history.history['val_auc'])

        fold_val_loss, fold_val_acc, fold_val_auc = model.evaluate(valid_gen_fold)
        print(f"Fold {fold_no} - Validation Loss: {fold_val_loss:.4f}, Validation Accuracy: {fold_val_acc:.4f}, Validation AUC: {fold_val_auc:.4f}")

        loss_per_fold.append(fold_val_loss)
        acc_per_fold.append(fold_val_acc)
        auc_per_fold.append(fold_val_auc)

        y_pred = model.predict(valid_gen_fold)
        y_true = np.array(valid_gen_fold.classes)
        class_names = list(valid_gen_fold.class_indices.keys())

        fpr = dict()
        tpr = dict()
        roc_auc = dict()

        for i in range(len(class_indices)):
            y_true_binary = np.array(y_true == i, dtype=int)
            fpr[i], tpr[i], _ = roc_curve(
                y_true_binary,
                y_pred[:, i]
            )
            roc_auc[i] = auc(fpr[i], tpr[i])

        fpr["micro"], tpr["micro"], _ = roc_curve(
            tf.keras.utils.to_categorical(y_true, num_classes=len(class_indices)).ravel(),
            y_pred.ravel()
        )
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(class_indices))]))
        mean_tpr = np.zeros_like(all_fpr)
        for i in range(len(class_indices)):
            mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
        mean_tpr /= len(class_indices)
        fpr["macro"] = all_fpr
        tpr["macro"] = mean_tpr
        roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

        roc_curves['fold'].append(fold_no)
        roc_curves['fpr'].append(fpr)
        roc_curves['tpr'].append(tpr)
        roc_curves['roc_auc'].append(roc_auc)

        lesion_names = [lesion_type_dict[key] for key in class_names]
        plot_roc_curves(fpr, tpr, roc_auc, lesion_names, f"{model_name} - ROC Curves for Fold {fold_no}")

        del model
        tf.keras.backend.clear_session()
        gc.collect()

        fold_no += 1

    print(f"\nModel {model_name} - CV Validation Accuracies for each fold: {acc_per_fold}")
    print(f"Model {model_name} - Mean Validation Accuracy: {np.mean(acc_per_fold):.4f}")
    print(f"Model {model_name} - CV Validation AUC for each fold: {auc_per_fold}")
    print(f"Model {model_name} - Mean Validation AUC: {np.mean(auc_per_fold):.4f}")

    avg_train_loss = np.mean(np.array(train_loss_histories), axis=0)
    avg_train_accuracy = np.mean(np.array(train_acc_histories), axis=0)
    avg_train_auc = np.mean(np.array(train_auc_histories), axis=0)
    avg_val_loss = np.mean(np.array(val_loss_histories), axis=0)
    avg_val_accuracy = np.mean(np.array(val_acc_histories), axis=0)
    avg_val_auc = np.mean(np.array(val_auc_histories), axis=0)
    epochs_range = range(1, num_epochs + 1)

    plt.figure(figsize=(18, 5))

    plt.subplot(1, 3, 1)
    plt.plot(epochs_range, avg_train_loss, label='Training Loss')
    plt.plot(epochs_range, avg_val_loss, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title(f'{model_name} - Loss Across Folds')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 3, 2)
    plt.plot(epochs_range, avg_train_accuracy, label='Training Accuracy')
    plt.plot(epochs_range, avg_val_accuracy, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title(f'{model_name} - Accuracy Across Folds')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 3, 3)
    plt.plot(epochs_range, avg_train_auc, label='Training AUC')
    plt.plot(epochs_range, avg_val_auc, label='Validation AUC')
    plt.xlabel('Epochs')
    plt.ylabel('AUC')
    plt.title(f'{model_name} - AUC Across Folds')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

"""# Final Training and Evaluation"""

for model_name, config in model_configs.items():
    print("\n================================================")
    print(f"Final Training and Evaluation for model: {model_name}")
    print("================================================")

    img_size = config["img_size"]
    use_rescale = config["use_rescale"]
    model_func = config["model_func"]

    print("\nBalancing full training data for final model...")
    balanced_df_train = balance_data(df_train, oversample=USE_AUGMENTATION_AND_OVERSAMPLING)
    print(f"Final balanced full training set size: {len(balanced_df_train)} samples")

    class_weights = calculate_class_weights(balanced_df_train['label_numeric'])
    print("Final model class weights:", class_weights)

    if use_rescale:
        train_gen_full = tr_gen_rescale.flow_from_dataframe(
            balanced_df_train,
            x_col='path',
            y_col='label',
            target_size=img_size,
            class_mode='categorical',
            color_mode='rgb',
            shuffle=True,
            batch_size=batch_size
        )
        test_gen = ts_gen_rescale.flow_from_dataframe(
            df_test,
            x_col='path',
            y_col='label',
            target_size=img_size,
            class_mode='categorical',
            color_mode='rgb',
            shuffle=False,
            batch_size=batch_size
        )
    else:
        train_gen_full = tr_gen_no_rescale.flow_from_dataframe(
            balanced_df_train,
            x_col='path',
            y_col='label',
            target_size=img_size,
            class_mode='categorical',
            color_mode='rgb',
            shuffle=True,
            batch_size=batch_size
        )
        test_gen = ts_gen_no_rescale.flow_from_dataframe(
            df_test,
            x_col='path',
            y_col='label',
            target_size=img_size,
            class_mode='categorical',
            color_mode='rgb',
            shuffle=False,
            batch_size=batch_size
        )

    num_classes = len(train_gen_full.class_indices)
    model = model_func(input_shape=(img_size[0], img_size[1], 3), num_classes=num_classes)
    model = compile_model(model)

    final_lr_reduction = ReduceLROnPlateau(
        monitor='accuracy',
        patience=3,
        verbose=1,
        factor=0.5,
        min_lr=0.00001
    )

    print("\n--- Final training on full training data ---")
    history_full = model.fit(
        train_gen_full,
        epochs=num_epochs,
        callbacks=[final_lr_reduction],
        class_weight=class_weights,
        verbose=1
    )

    plot_training_history(history_full, model_name)

    print("\nEvaluating on test set...")
    score = model.evaluate(test_gen)
    loss, accuracy, auc_value = score
    print(f"\nTest Loss for {model_name}: {loss:.4f}")
    print(f"Test Accuracy for {model_name}: {accuracy:.4f}")
    print(f"Test AUC for {model_name}: {auc_value:.4f}")

    print("\nGenerating predictions for ROC curves...")
    y_pred = model.predict(test_gen)
    y_true = np.array(test_gen.classes)
    class_names = list(test_gen.class_indices.keys())

    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i in range(num_classes):
        y_true_binary = np.array(y_true == i, dtype=int)
        fpr[i], tpr[i], _ = roc_curve(
            y_true_binary,
            y_pred[:, i]
        )
        roc_auc[i] = auc(fpr[i], tpr[i])

    fpr["micro"], tpr["micro"], _ = roc_curve(
        tf.keras.utils.to_categorical(y_true, num_classes=num_classes).ravel(),
        y_pred.ravel()
    )
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(num_classes)]))
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(num_classes):
        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
    mean_tpr /= num_classes
    fpr["macro"] = all_fpr
    tpr["macro"] = mean_tpr
    roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

    lesion_names = [lesion_type_dict[key] for key in class_names]
    plot_roc_curves(fpr, tpr, roc_auc, lesion_names, f"{model_name} - Test Set ROC Curves")

    print("\nGenerating confusion matrix...")
    predictions = np.argmax(y_pred, axis=1)
    cm = confusion_matrix(y_true, predictions)

    plt.figure(figsize=(10,10))
    sns.set(font_scale=1.2)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=[lesion_type_dict[key] for key in class_names],
                yticklabels=[lesion_type_dict[key] for key in class_names])
    plt.title(f'Confusion Matrix for {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    print("\nClassification Report:")
    print(classification_report(
        y_true,
        predictions,
        target_names=[lesion_type_dict[key] for key in class_names],
        zero_division=0
    ))

    del model
    tf.keras.backend.clear_session()
    gc.collect()